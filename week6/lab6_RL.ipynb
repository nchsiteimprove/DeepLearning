{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning Exercise\n",
    "\n",
    "For this exercise we will be using the [OpenAI Gym](https://gym.openai.com/) provided by [OpenAI](https://openai.com/). To get to know Gym, you are encouraged to read this [blogpost](https://openai.com/blog/openai-gym-beta/) (~10 minutes) and refer to the [docs](https://gym.openai.com/docs) along the way.\n",
    "\n",
    "In this exercise we will train a neural network agent to navigate various environments from the OpenAI Gym.\n",
    "\n",
    "## 0. Prerequisites\n",
    "\n",
    "We assume you already have Theano and Lasagne installed -- otherwise go back to the first exercise for instructions. \n",
    "\n",
    "Below is a brief guide on how to install OpenAI Gym. For more details please refer to the [docs](https://gym.openai.com/docs).\n",
    "   \n",
    "```\n",
    "$ cd ~/path/to/dir/...\n",
    "$ git clone https://github.com/openai/gym\n",
    "$ cd gym\n",
    "$ pip install -e . # minimal install\n",
    "```\n",
    "\n",
    "Verify your installation is working by importing `gym` and check for errors:\n",
    "\n",
    "```\n",
    "$ python\n",
    ">>> import gym\n",
    "[no errors]\n",
    "```\n",
    "\n",
    "Now restart this notebook before moving on to the next part."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Getting started\n",
    "\n",
    "Now that you have everythong installed, lets get started!\n",
    "\n",
    "The code below will import `gym` and initialize the [CartPole-v0](https://gym.openai.com/envs/CartPole-v0) environment. The task of this environment is to move a cart in order to balance a pole attached on top, but for now we will just take random actions for 200 timesteps to see what happens. The environment is rendered in a separate window so you can observe the result. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2016-10-10 09:46:26,642] Making new env: CartPole-v0\n",
      "[2016-10-10 09:46:27,536] You are calling 'step()' even though this environment has already returned done = True. You should always call 'reset()' once you receive 'done = True' -- any further steps are undefined behavior.\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import time\n",
    "\n",
    "# init and run an example environment\n",
    "env = gym.make('CartPole-v0')\n",
    "env.reset()\n",
    "for _ in range(200):\n",
    "    env.render()\n",
    "    env.step(env.action_space.sample())\n",
    "    time.sleep(0.05) ### Needed to slow things down to see what happens\n",
    "env.render(close=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That was all very nice, but taking random actions doesn't really solve the task. We have to do something smarter. In the next part we will train an agent to solve the task by reinforcement learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Policy gradient agent\n",
    "\n",
    "In this part we will create an agent that can learn to solve tasks from OpenAI Gym by applying the policy gradient method.\n",
    "\n",
    "The agent is designed to work on environments with a discrete action space. Extending the code to also handle environments with countinous action space is left as an optional exercise.\n",
    "\n",
    "But first here is a short introduction to policy gradients.\n",
    "\n",
    "### Policy gradients\n",
    "\n",
    "We want to learn a policy neural network $p_\\theta(a_{t}|s_{t-1})$ with parameters $\\theta$ for action $a_t$ given the previous state $s_{t-1}$ only.\n",
    "When the action $a$ is discrete we can implement this by a softmax output taking $s$ as input. \n",
    "The (discounted) cumulative award for a sequence terminating after $T$ time-steps is\n",
    "\n",
    "$$\n",
    "R = \\sum_{t=1}^T \\gamma^{t-1} r_{t} \\ .\n",
    "$$\n",
    "\n",
    "The expectation of $R$ over a\n",
    "policy roll-out $p_\\theta({\\bf a}|{\\bf s})$ is \n",
    "\n",
    "$$\n",
    "\\mathbb{E}[R|\\theta] = \\int R({\\bf a},{\\bf s}) p_\\theta({\\bf a},{\\bf s}) d{\\bf a} d{\\bf s}\\ ,\n",
    "$$\n",
    "\n",
    "where ${\\bf a} = a_1,\\ldots,a_T$, ${\\bf s}=s_0,\\ldots,s_T$ and\n",
    "\n",
    "$$\n",
    "p_\\theta({\\bf a},{\\bf s}) = p(s_0) \\prod_{t=1}^T \\left[ \n",
    "p(s_{t}|s_{t-1},a_t) p_\\theta(a_{t}|s_{t-1})\n",
    "\\right]\\ .\n",
    "$$\n",
    "\n",
    "In this formulation $s_t$ is a stochastic function of the previous action and state: $p(s_t|a_t,s_{t-1})$. We can draw from the joint distribution of actions and states through the environment but $p(s_t|a_t,s_{t-1})$ is unknown. \n",
    "\n",
    "A deterministic environment, think chess or go, is a special case of this set-up where the state is a deterministic function of the previous state and action: $s_t = f(a_t,s_{t-1})$. We can include the deterministic formulation in the general by using a Dirac $\\delta$-function: $p(s_t|a_t,s_{t-1})= \\delta(s_t - f(a_t,s_{t-1}))$. The integration over $s_1,\\ldots,s_T$ may be carried out explicitly:    \n",
    "\n",
    "$$\n",
    "\\mathbb{E}[R|\\theta] = \\int R({\\bf a},s_0) p(s_0) \\prod_{t=1}^T  p_\\theta(a_{t}|s_{t-1})d{\\bf a} ds_0\\ ,\n",
    "$$\n",
    "\n",
    "where $s_{t-1}=f(a_{t-1},s_{t-2})$.\n",
    "\n",
    "We use gradient ascent to learn an approximation to a policy that maximizes the cumulative reward. \n",
    "So we need to compute the gradient:\n",
    "\n",
    "$$\n",
    "\\nabla_\\theta \\mathbb{E}[R|\\theta] = \\int R({\\bf a}, {\\bf s}) \\nabla_\\theta p_\\theta({\\bf a},{\\bf s}) \\, d{\\bf a}d{\\bf s} \\ .\n",
    "$$\n",
    "\n",
    "We can now use the identity\n",
    "\n",
    "$$\n",
    "\\nabla_\\theta p_\\theta({\\bf a},{\\bf s}) = p_\\theta({\\bf a},{\\bf s}) \\nabla_\\theta \\log p_\\theta({\\bf a},{\\bf s})\n",
    "$$\n",
    "\n",
    "to express the gradient as an average over $p_\\theta({\\bf a},{\\bf s})$:\n",
    "\n",
    "$$\n",
    "\\nabla_\\theta \\mathbb{E}[R|\\theta] = \\int p_\\theta({\\bf a},{\\bf s}) ( R({\\bf a}, {\\bf s}) - b ) \\nabla_\\theta \\log p_\\theta({\\bf a},{\\bf s}) d{\\bf a}d{\\bf s}\\ .\n",
    "$$\n",
    "\n",
    "The constant factor $b$ will not affect the gradient but will needed in practice when we estimate gradients by Monte Carlo (that is roll-outs). We can prove that subtracting $b$ will not change the gradient by using the identity from above again:\n",
    "\n",
    "\\begin{align*}\n",
    "0 & = \\nabla_\\theta 1 \\\\\n",
    "  & = \\nabla_\\theta \\int p_\\theta({\\bf a},{\\bf s}) \\, d{\\bf a} d{\\bf s}\\\\\n",
    "  & = \\int \\nabla_\\theta p_\\theta({\\bf a},{\\bf s}) \\, d{\\bf a}d{\\bf s}\\\\\n",
    "  & = \\int p_\\theta({\\bf a},{\\bf s}) \\nabla_\\theta \\log p_\\theta({\\bf a},{\\bf s}) \\, d{\\bf a}d{\\bf s} \\ .\n",
    "\\end{align*}\n",
    "\n",
    "We cannot evaluate the average over roll-outs analytically but we have an environment simulator that when supplied with our current policy $p_\\theta(a|s)$ can return the sequence of action, states and rewards. This allows us to replace the integral by a Monte Carlo average over $V$ roll-outs\n",
    "\n",
    "$$\n",
    "\\nabla_\\theta \\mathbb{E}[R|\\theta] \\approx \\frac{1}{V} \\sum_{v=1}^V ( R({\\bf a}^{(v)}, {\\bf s}^{(v)}) - b) \\nabla_\\theta \\log p_\\theta({\\bf a}^{(v)},{\\bf s}^{(v)})\n",
    "$$\n",
    "\n",
    "Note also that the gradient of $\\log p_\\theta({\\bf a}^{(v)},{\\bf s}^{(v)})$ does not depend explicitly on the state distribution:\n",
    "\n",
    "$$\n",
    "\\nabla_\\theta \\log p_\\theta({\\bf a}^{(v)},{\\bf s}^{(v)}) = \\sum_{t=1}^T \\nabla_\\theta \\log p_\\theta(a_{t}|s_{t-1}) \\ .\n",
    "$$\n",
    "\n",
    "We are almost done. As a last step we will use the freedom in the choice of $b$ to select a $b$ that will make the Monte Carlo estimate of the gradient have the lowest possible variance. In other words, the finite Monte Carlo sample give us a noisy gradient and by this correction we can make it vary as little as possible between roll-out draws.\n",
    "\n",
    "The $b$ that minimizes the variances can be found from minimizing the following expression:\n",
    "\n",
    "$$\n",
    "\\int p_\\theta({\\bf a},{\\bf s}) \\, ( R({\\bf a}, {\\bf s}) - b )^2 \\,  |\\nabla_\\theta \\log p_\\theta({\\bf a},{\\bf s})|^2 \\,  d{\\bf a}d{\\bf s} - \\left( \\nabla_\\theta \\mathbb{E}[R|\\theta] \\right)^2 \\ .\n",
    "$$\n",
    "\n",
    "The solution to this problem is\n",
    "\n",
    "$$\n",
    "b = \\frac{\\int p_\\theta({\\bf a},{\\bf s}) \\, R({\\bf a}, {\\bf s}) \\,  |\\nabla_\\theta \\log p_\\theta({\\bf a},{\\bf s})|^2 d{\\bf a}d{\\bf s}}{\\int p_\\theta({\\bf a},{\\bf s}) \\, |\\nabla_\\theta \\log p_\\theta({\\bf a},{\\bf s})|^2 d{\\bf a}d{\\bf s}} \\ .\n",
    "$$\n",
    "\n",
    "We replace this expression by a Monte Carlo average:\n",
    "\n",
    "$$\n",
    "b = \\frac{\\sum_{v=1}^V  R({\\bf a}^{(v)}, {\\bf s}^{(v)}) \\, |\\nabla_\\theta \\log p_\\theta({\\bf a}^{(v)},{\\bf s}^{(v)})|^2}{\\sum_{v=1}^V | \\nabla_\\theta \\log p_\\theta({\\bf a}^{(v)},{\\bf s}^{(v)})|^2}\n",
    "$$\n",
    "\n",
    "In the code below we instead use a time-step dependent baseline correction, $b(s_t)$, as described in [here](https://gym.openai.com/docs/rl#policy-gradients)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "import lasagne\n",
    "from lasagne.layers import InputLayer, DenseLayer, DropoutLayer\n",
    "from lasagne.nonlinearities import tanh, softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Agent(object):\n",
    "    \"\"\"\n",
    "    Reinforcement Learning Agent\n",
    "    \n",
    "    This agent can learn to solve reinforcement learning tasks from\n",
    "    OpenAI Gym by applying the policy gradient method.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_inputs, n_outputs):\n",
    "        # symbolic variables for state, action, and advantage\n",
    "        sym_state = T.fmatrix()\n",
    "        sym_action = T.ivector()\n",
    "        sym_advantage = T.fvector()\n",
    "        # policy network\n",
    "        n_units = 20 #Default = 20\n",
    "        l_in = InputLayer(shape=(None, n_inputs))\n",
    "        l_hid = DenseLayer(incoming=l_in, num_units=n_units, nonlinearity=tanh, name='hiddenlayer')\n",
    "        l_drop = DropoutLayer(incoming=l_hid)\n",
    "        l_hid2 = DenseLayer(incoming=l_drop, num_units=n_units, nonlinearity=tanh, name='hiddenlayer2')\n",
    "        l_drop2 = DropoutLayer(incoming=l_hid2)\n",
    "#         l_hid3 = DenseLayer(incoming=l_drop2, num_units=n_units, nonlinearity=tanh, name='hiddenlayer3')\n",
    "#         l_drop3 = DropoutLayer(incoming=l_hid3)\n",
    "#         l_hid4 = DenseLayer(incoming=l_drop3, num_units=n_units, nonlinearity=tanh, name='hiddenlayer4')\n",
    "#         l_drop4 = DropoutLayer(incoming=l_hid4)\n",
    "        l_out = DenseLayer(incoming=l_drop2, num_units=n_outputs, nonlinearity=softmax, name='outputlayer')\n",
    "        # get network output\n",
    "        eval_out = lasagne.layers.get_output(l_out, {l_in: sym_state}, deterministic=True)\n",
    "        # get trainable parameters in the network.\n",
    "        params = lasagne.layers.get_all_params(l_out, trainable=True)\n",
    "        # get total number of timesteps\n",
    "        t_total = sym_state.shape[0]\n",
    "        # loss function that we'll differentiate to get the policy gradient\n",
    "        loss = -T.log(eval_out[T.arange(t_total), sym_action]).dot(sym_advantage) / t_total\n",
    "        # learning_rate\n",
    "        learning_rate = T.fscalar()\n",
    "        # get gradients\n",
    "        grads = T.grad(loss, params)\n",
    "        # update function\n",
    "        ### Changed from sgd\n",
    "        updates = lasagne.updates.adam(grads, params, learning_rate=learning_rate)\n",
    "        # declare training and evaluation functions\n",
    "        self.f_train = theano.function([sym_state, sym_action, sym_advantage, learning_rate], loss, updates=updates, allow_input_downcast=True)\n",
    "        self.f_eval = theano.function([sym_state], eval_out, allow_input_downcast=True)\n",
    "    \n",
    "    def learn(self, env, n_epochs=100, t_per_batch=10000, traj_t_limit=None,\n",
    "              learning_rate=0.1, discount_factor=1.0, n_early_stop=0):\n",
    "        \"\"\"\n",
    "        Learn the given environment by the policy gradient method.\n",
    "        \"\"\"\n",
    "        self.mean_train_rs = []\n",
    "        self.mean_val_rs = []\n",
    "        self.loss = []\n",
    "        for epoch in xrange(n_epochs):\n",
    "            # 1. collect trajectories until we have at least t_per_batch total timesteps\n",
    "            ### Generate training data by simulating trajectories. \n",
    "            ### The length of each trajectory is the number of timesteps \n",
    "            ### the agent suceeds in balancing the pole.\n",
    "            ### t_total timesteps are simulated in total.\n",
    "            trajs = []; t_total = 0\n",
    "            while t_total < t_per_batch:\n",
    "                traj = self.get_trajectory(env, traj_t_limit, deterministic=False)\n",
    "                trajs.append(traj)\n",
    "                t_total += len(traj[\"r\"])\n",
    "            all_s = np.concatenate([traj[\"s\"] for traj in trajs])\n",
    "            # 2. compute cumulative discounted rewards (returns)\n",
    "            ### Compute the cumulative discount for the trajectories \n",
    "            ### created above. Note: the implementation is not the same\n",
    "            ### as the formula at the top of the notebook, but it has\n",
    "            ### the same behaviour. \n",
    "            ### Pad all but the longest trajectory with 0's to represent\n",
    "            ### timesteps where the pole is no longer upright.\n",
    "            rets = [self._cumulative_discount(traj[\"r\"], discount_factor) for traj in trajs]\n",
    "            maxlen = max(len(ret) for ret in rets)\n",
    "            padded_rets = [np.concatenate([ret, np.zeros(maxlen-len(ret))]) for ret in rets]\n",
    "            # 3. compute time-dependent baseline\n",
    "            ### Calculate the mean for each timestep. Used later\n",
    "            ### to determine which timesteps are good (above the mean).\n",
    "            baseline = np.mean(padded_rets, axis=0)\n",
    "            # 4. compute advantages\n",
    "            ### Calculate advantages, which are indicators of how good\n",
    "            ### each move at a certain timestep is compared to the mean.\n",
    "            ### Also flatten advs (along with states and actions) \n",
    "            ### to be able to feed them to the network.\n",
    "            advs = [ret - baseline[:len(ret)] for ret in rets]\n",
    "            all_a = np.concatenate([traj[\"a\"] for traj in trajs])\n",
    "            all_adv = np.concatenate(advs)\n",
    "            # 5. do policy gradient update step\n",
    "            ### Train the network and calculate the loss using the loss function defined earlier\n",
    "            loss = self.f_train(all_s, all_a, all_adv, learning_rate)\n",
    "            train_rs = np.array([traj[\"r\"].sum() for traj in trajs]) # trajectory total rewards\n",
    "            ### Not used...\n",
    "            eplens = np.array([len(traj[\"r\"]) for traj in trajs]) # trajectory lengths\n",
    "            # compute validation reward\n",
    "            ### Validate the network on 10 deterministic simulations (take best action each step).\n",
    "            ### We need 10 simulations because the start state is random. We use the mean as valid score\n",
    "            val_rs = np.array([self.get_trajectory(env, traj_t_limit, deterministic=True)['r'].sum() for _ in range(10)])\n",
    "            # update stats\n",
    "            self.mean_train_rs.append(train_rs.mean())\n",
    "            self.mean_val_rs.append(val_rs.mean())\n",
    "            self.loss.append(loss)\n",
    "            # print stats\n",
    "            print '%3d mean_train_r: %6.2f mean_val_r: %6.2f loss: %f' % (epoch+1, train_rs.mean(), val_rs.mean(), loss)\n",
    "            # render solution\n",
    "            #self.get_trajectory(env, traj_t_limit, render=True)\n",
    "            # check for early stopping: true if the validation reward has not changed in n_early_stop epochs\n",
    "            if n_early_stop and len(self.mean_val_rs) >= n_early_stop and \\\n",
    "                all([x == self.mean_val_rs[-1] for x in self.mean_val_rs[-n_early_stop:-1]]):\n",
    "                break\n",
    "    \n",
    "    def get_trajectory(self, env, t_limit=None, render=False, deterministic=True):\n",
    "        \"\"\"\n",
    "        Compute trajectroy by iteratively evaluating the agent policy on the environment.\n",
    "        \"\"\"\n",
    "        t_limit = t_limit or env.spec.timestep_limit\n",
    "        s = env.reset()\n",
    "        traj = {'s': [], 'a': [], 'r': [],}\n",
    "        for _ in xrange(t_limit):\n",
    "            a = self.get_action(s, deterministic)\n",
    "            (s, r, done, _) = env.step(a)\n",
    "            traj['s'].append(s)\n",
    "            traj['a'].append(a)\n",
    "            traj['r'].append(r)\n",
    "            if render: \n",
    "                import time\n",
    "                time.sleep(0.07)\n",
    "                env.render()\n",
    "            if done: break\n",
    "        return {'s': np.array(traj['s']), 'a': np.array(traj['a']), 'r': np.array(traj['r'])}\n",
    "    \n",
    "    def get_action(self, s, deterministic=True):\n",
    "        \"\"\"\n",
    "        Evaluate the agent policy to choose an action, a, given state, s.\n",
    "        \"\"\"\n",
    "        # compute action probabilities\n",
    "        prob_a = self.f_eval(s.reshape(1,-1))\n",
    "        if deterministic:\n",
    "            # choose action with highest probability\n",
    "            return prob_a.argmax()\n",
    "        else:\n",
    "            # sample action from distribution\n",
    "            return (np.cumsum(np.asarray(prob_a)) > np.random.rand()).argmax()\n",
    "    \n",
    "    def _cumulative_discount(self, r, gamma):\n",
    "        \"\"\"\n",
    "        Compute the cumulative discounted rewards (returns).\n",
    "        \"\"\"\n",
    "        r_out = np.zeros(len(r), 'float64')\n",
    "        r_out[-1] = r[-1]\n",
    "        for i in reversed(xrange(len(r)-1)):\n",
    "            r_out[i] = r[i] + gamma * r_out[i+1]\n",
    "        return r_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have an agent, let's train it to solve the CartPole task.\n",
    "\n",
    "Note: The agent is not guaranteed to learn a good solution every time as the policy gradient method might get stuck in a local optimum -- you may have to do several restarts to find a good solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2016-10-10 12:47:05,504] Making new env: CartPole-v0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  1 mean_train_r:  35.73 mean_val_r: 188.00 loss: 28.068041\n",
      "  2 mean_train_r:  35.68 mean_val_r:  97.20 loss: 16.444902\n",
      "  3 mean_train_r:  36.37 mean_val_r: 168.90 loss: 14.422903\n",
      "  4 mean_train_r:  34.84 mean_val_r: 280.80 loss: 13.587684\n",
      "  5 mean_train_r:  35.81 mean_val_r: 121.10 loss: 15.716883\n",
      "  6 mean_train_r:  34.63 mean_val_r: 162.60 loss: 14.328050\n",
      "  7 mean_train_r:  36.24 mean_val_r: 198.60 loss: 41.342733\n",
      "  8 mean_train_r:  37.41 mean_val_r: 189.30 loss: 22.866348\n",
      "  9 mean_train_r:  35.67 mean_val_r: 239.00 loss: 134.231514\n",
      " 10 mean_train_r:  34.82 mean_val_r: 217.70 loss: 13.959138\n",
      " 11 mean_train_r:  36.44 mean_val_r: 178.40 loss: 97.290498\n",
      " 12 mean_train_r:  37.05 mean_val_r: 235.50 loss: 20.909477\n",
      " 13 mean_train_r:  33.52 mean_val_r: 198.70 loss: 13.864835\n",
      " 14 mean_train_r:  36.11 mean_val_r: 163.90 loss: 62.334436\n",
      " 15 mean_train_r:  33.23 mean_val_r: 206.10 loss: 15.832184\n",
      " 16 mean_train_r:  34.38 mean_val_r: 191.10 loss: 38.084228\n",
      " 17 mean_train_r:  34.07 mean_val_r: 202.60 loss: 18.382917\n",
      " 18 mean_train_r:  35.98 mean_val_r: 190.90 loss: 67.089333\n",
      " 19 mean_train_r:  36.06 mean_val_r: 215.00 loss: 15.797143\n",
      " 20 mean_train_r:  34.70 mean_val_r: 270.90 loss: 20.364384\n",
      " 21 mean_train_r:  34.80 mean_val_r: 300.00 loss: 19.697087\n",
      " 22 mean_train_r:  32.99 mean_val_r: 300.00 loss: 14.880508\n",
      " 23 mean_train_r:  35.06 mean_val_r: 300.00 loss: 17.390169\n",
      " 24 mean_train_r:  34.58 mean_val_r: 300.00 loss: 19.752707\n",
      " 25 mean_train_r:  33.34 mean_val_r: 300.00 loss: 53.890619\n"
     ]
    }
   ],
   "source": [
    "# init environment\n",
    "env = gym.make('CartPole-v0')\n",
    "# init agent\n",
    "agent = Agent(n_inputs=env.observation_space.shape[0],\n",
    "              n_outputs=env.action_space.n)\n",
    "traj_t = 300#env.spec.timestep_limit\n",
    "# train agent on the environment\n",
    "### Default params: n_epochs=100, learning_rate=0.05, \n",
    "### discount_factor=1, t_per_batch=10000, \n",
    "### traj_t_limit=env.spec.timestep_limit, n_early_stop=5\n",
    "agent.learn(env, n_epochs=100, learning_rate=0.00091, discount_factor=1.05,\n",
    "            t_per_batch=20000, traj_t_limit=traj_t, n_early_stop=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAmIAAAFHCAYAAAAC3yD+AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nqMpZTM1gQwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f279adf19d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot training and validation mean reward\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.xlabel('epochs'); plt.ylabel('mean reward')\n",
    "plt.plot(agent.mean_train_rs, label='training')\n",
    "plt.plot(agent.mean_val_rs, label='validation')\n",
    "plt.xlim((0,len(agent.mean_val_rs)-1))\n",
    "plt.legend(loc=2); plt.grid()\n",
    "_=plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# review solution\n",
    "agent.get_trajectory(env, t_limit=1000, render=True)\n",
    "env.render(close=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The initial solution does not learn to solve the task very well. Here are some hints on how to improve the solution:\n",
    "\n",
    "* Increase the trajectory timestep limit to let the simulations look further into the future.\n",
    "* Increase number of timesteps evaluated per batch.\n",
    "* Try different optimization functions.\n",
    "* Adjust the learning rate.\n",
    "* Adjust the discount factor.\n",
    "\n",
    "### Exercises\n",
    "\n",
    "1. Describe the changes you made and and why they should improve the agent. Are you able to get solutions consistently?\n",
    "2. In the plot above you will sometimes see that the validation reward starts out lower than the training reward but later they cross. How can you explain this behavior?\n",
    "3. Explain step by step the algorithm in the `agent.learn` method with particular attention the points denoted 1-5 in the code above.\n",
    "4. Optional: Monitor and submit your best solution to the Gym (see code below).\n",
    "\n",
    "### Answers\n",
    "1. \n",
    "    - Changed from sgd to Adam to gain benefit of momentum, hopefully avoiding local minima. Reduced learning rate as well.\n",
    "    - Added a dropout layer after the hidden layer to reduce overfitting.\n",
    "    - Changed traj_t to allow the network to look further ahead. My reasoning was that training on 200 timesteps (default) and generalizing to 1000 is harder than training on 500 and generalizing to 1000.\n",
    "    - Doubled t_per_batch to allow more simulated trajectories.\n",
    "    - Reduced discount factor below 1 to put more weight on early actions. The reason being that if the system never destabilizes the pole too much it will be easier to balance indefinitely. \n",
    "    - Increased discount above 1 to put more weight on later actions, making the system able to better correct itself to stabilize the pole not matter how many timesteps have elapsed.\n",
    "    - Decreased learning rate further. Finally began to see somewhat consistent results wrt the valid rewards, but it still cannot consistently solve the task.\n",
    "2. When the network is really bad the validation actions, which are always the most likely, will consistently be the wrong actions. Even when the network is really bad, the train actions have a chance to be the right ones because of its semi-random action decision.\n",
    "When the network starts to improve, the validation actions will now move closer to the right actions at each step, while the train actions retain their chance to pick something that is against the most likely one (the unlikely one now moving towards being the wrong ones). This means that as the network improves we will see a greater reward for validation actions as they approach the right actions to take at any given time.\n",
    "3. See comments in code above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# start monitor\n",
    "#env.monitor.start('cartpole-experiment-1')\n",
    "#for _ in xrange(100):\n",
    "#    agent.get_trajectory(env)\n",
    "#env.monitor.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you have monitored a solution open a Python shell and use the following command to upload the results to OpenAI Gym:\n",
    "\n",
    "```\n",
    "import gym\n",
    "gym.upload('cartpole-experiment-1', api_key='YOUR_API_KEY')\n",
    "```\n",
    "\n",
    "You can also run the command here in the notebook, but remember to remove the API key before handing in the exercise.\n",
    "\n",
    "You can find your API key at your [OpenAI Gym](https://gym.openai.com/) account page. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Playground\n",
    "\n",
    "In this part you can try to learn a solution to an environment from [OpenAI Gym](https://gym.openai.com/) of your own choice. Some environments will require you to adjust the agent code since they have different properties, however, some environments such as 'Acrobot-v1' and 'MountainCar-v0' should work out of the box.\n",
    "\n",
    "Below is some code to get you started.\n",
    "\n",
    "### Exercises\n",
    "\n",
    "1. Describe the environment you have chosen including its state and action space.\n",
    "2. Describe the particular challanges of the environment you have chosen.\n",
    "3. Were you able to train a good solution? How/why not?\n",
    "4. Optional: Monitor and submit your solution to the Gym.\n",
    "\n",
    "### Answers\n",
    "After spending a long time getting stable results on the first part of the assignment, I did not have the time to try this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# init environment\n",
    "#my_env = gym.make('some-environment')\n",
    "# init agent\n",
    "#my_agent = Agent(n_inputs=my_env.observation_space.shape[0],\n",
    "#              n_outputs=my_env.action_space.n)\n",
    "# train agent on the environment\n",
    "#my_agent.learn(my_env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# plot training and validation mean reward\n",
    "#plt.figure(figsize=(10,5))\n",
    "#plt.xlabel('epochs'); plt.ylabel('mean reward')\n",
    "#plt.plot(my_agent.mean_train_rs, label='training')\n",
    "#plt.plot(my_agent.mean_val_rs, label='validation')\n",
    "#plt.xlim((0,len(my_agent.mean_val_rs)-1))\n",
    "#plt.legend(loc=2); plt.grid()\n",
    "#_=plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# review solution\n",
    "#my_agent.get_trajectory(my_env, t_limit=1000, render=True)\n",
    "#my_env.render(close=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Optional exercises\n",
    "\n",
    "1. Optional: Extend the agent to also work on environments with a continous action space."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
